{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征转换（Feature Transformation）\n",
    "\n",
    "在这个笔记本中，我们将原始文本转换为经过分词的输入，这些输入可以被HuggingFace训练脚本使用。将这个分词和提示创建的步骤与训练过程分开是非常重要的，因为这样可以根据每个步骤的需求选择最高效的计算资源。例如，在工作流程的准备阶段，通常使用成本较低的CPU处理器效果最佳，而在模型训练阶段，使用成本较高的GPU实例效果最佳。\n",
    "\n",
    "重要的是，在准备数据的分词和提示创建步骤中，使用低成本的CPU处理器可以节省计算成本。而对于模型训练这样需要更多计算资源的任务，则使用成本较高的GPU实例更为适合。GPU对于训练深度学习模型非常有效，可以显著加速训练过程。\n",
    "\n",
    "通过将分词和提示创建步骤与训练步骤分开，您可以更高效地分配计算资源，充分利用低成本的CPU处理器和高成本的GPU实例，优化整个工作流程，并在可能的情况下降低成本。\n",
    "\n",
    "![Pipeline](./img/generative_ai_pipeline_rlhf_plus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "## Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"img/kernel_set_up.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"img/w3_kernel_and_instance_type.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=33229983744, available=27910823936, percent=16.0, used=4949753856, free=400433152, active=3906199552, inactive=25742184448, buffers=1634304, cached=27878162432, shared=1200128, slab=2496040960)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "notebook_memory = psutil.virtual_memory()\n",
    "print(notebook_memory)\n",
    "\n",
    "if notebook_memory.total < 32 * 1000 * 1000 * 1000:\n",
    "    print('*******************************************')    \n",
    "    print('YOU ARE NOT USING THE CORRECT INSTANCE TYPE')\n",
    "    print('PLEASE CHANGE INSTANCE TYPE TO  m5.2xlarge ')\n",
    "    print('*******************************************')\n",
    "else:\n",
    "    correct_instance_type=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r setup_dependencies_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    setup_dependencies_passed\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN THE PREVIOUS NOTEBOOK \")\n",
    "    print(\"You did not install the required libraries.   \")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入 transformers 库中的 AutoTokenizer。这个库主要用于自然语言处理，并且 AutoTokenizer 可以自动地处理各种预训练的模型的分词操作。\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 导入 datasets 库中的 load_dataset 和 DatasetDict。datasets 是一个用于处理和管理大规模数据集的库，load_dataset 能够加载各种格式的数据集，而 DatasetDict 则是一种用于处理多个数据集的高级字典。\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 导入 os 库。os 库提供了许多与操作系统交互的函数，例如读写文件、管理目录路径等。\n",
    "import os\n",
    "\n",
    "# 导入 time 库。time 库提供了各种与时间有关的函数，例如获取当前时间、使程序暂停等。\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 确保基础数据集已下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n"
     ]
    }
   ],
   "source": [
    "# 检查 './data-summarization/' 目录是否存在。os.path.isdir 函数用于检查指定的路径是否为一个目录。\n",
    "if os.path.isdir('./data-summarization/'):\n",
    "    # 如果目录已经存在，打印 'Dataset already downloaded'，表示数据集已经下载。\n",
    "    print('Dataset already downloaded')\n",
    "else:\n",
    "    # 如果目录不存在，那么开始下载和处理数据集。\n",
    "    \n",
    "    # 从 datasets 库导入 concatenate_datasets 函数，该函数用于将多个数据集合并为一个数据集。\n",
    "    from datasets import concatenate_datasets\n",
    "    \n",
    "    # 使用 load_dataset 函数加载名为 \"knkarthick/dialogsum\" 的数据集。\n",
    "    dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "    \n",
    "    # 使用 concatenate_datasets 函数将数据集的 'train'、'test' 和 'validation' 部分合并为一个数据集。\n",
    "    dataset = concatenate_datasets([dataset['train'], dataset['test'], dataset['validation']])\n",
    "    \n",
    "    # 使用 shell 命令 'mkdir' 创建一个名为 'data-summarization' 的目录。\n",
    "    !mkdir data-summarization\n",
    "    \n",
    "    # 使用 train_test_split 函数将合并后的数据集分割为训练集和测试集，分割比例为 50%，并设置随机种子为 1234 以确保结果的可复现性。\n",
    "    dataset = dataset.train_test_split(0.5, seed=1234)\n",
    "    \n",
    "    # 将测试集保存为 CSV 文件，文件名为 'dialogsum-1.csv'，并放在 'data-summarization' 目录下。设置参数 index=False 表示不保存行索引。\n",
    "    dataset['test'].to_csv('./data-summarization/dialogsum-1.csv', index=False)\n",
    "    \n",
    "    # 将训练集保存为 CSV 文件，文件名为 'dialogsum-2.csv'，并放在 'data-summarization' 目录下。设置参数 index=False 表示不保存行索引。\n",
    "    dataset['train'].to_csv('./data-summarization/dialogsum-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载分词器（Tokenizer）和HuggingFace数据集（Dataset）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/data-summarization to /root/.cache/huggingface/datasets/csv/data-summarization-55f5a7efa2dfb458/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7258c82a39834f2bb70a55d1ea02aa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c062d227de49fc86850b7843608f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/data-summarization-55f5a7efa2dfb458/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca8b1a365164de39c769e875de402ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 14460\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 AutoTokenizer 的 from_pretrained 方法加载预训练的分词器。model_checkpoint 是预训练模型的名称或路径，这里是'google/flan-t5-base'。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 使用 load_dataset 函数加载 './data-summarization/' 路径下的数据集。\n",
    "dataset = load_dataset('./data-summarization/')\n",
    "\n",
    "# 打印加载的数据集。这将显示数据集的基本信息，包括其包含的样本数、特征名称等。\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浏览Prompt样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "--------------------------\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: If we employ you, what starting salary would you expect?\n",
      "#Person2#: I'd like to start at 3000 yuan a month.\n",
      "#Person1#: I think your background and experience are worth the compensation.\n",
      "#Person2#: Does it include bonuses?\n",
      "#Person1#: No, there are annual bonuses, one week paid vacation a year, and health insurance.\n",
      "#Person2#: Very good.\n",
      "\n",
      "Summary:\n",
      "--------------------------\n",
      "Baseline human summary : #Person1# agrees #Person2#'s starting monthly salary would be 3000 yuan and tells #Person2# about other benefits.\n"
     ]
    }
   ],
   "source": [
    "# 设置索引值为 0，这将用于从数据集中选取样本。\n",
    "idx = 0\n",
    "\n",
    "# 从 'train' 部分的第 idx 个样本中获取 'dialogue' 和 'summary' 字段的值。'dialogue' 字段包含了对话的内容，'summary' 字段包含了对话的摘要。\n",
    "diag = dataset['train'][idx]['dialogue']\n",
    "baseline_human_summary = dataset['train'][idx]['summary']\n",
    "\n",
    "# 构造模型的输入提示。输入提示包括对话的内容和一个摘要的开头，后面的模型将生成摘要的剩余部分。\n",
    "prompt = f'Summarize the following conversation.\\n\\n{diag}\\n\\nSummary:'\n",
    "\n",
    "# 使用预训练的分词器将输入提示转换为模型的输入格式。return_tensors=\"pt\" 表示返回 PyTorch 的张量格式。\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 打印输入提示和人工总结的摘要。\n",
    "print(f'Prompt:\\n--------------------------\\n{prompt}\\n--------------------------')\n",
    "print(f'Baseline human summary : {baseline_human_summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据集进行分词处理（Tokenize the Dataset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenize_function 函数将数据集的每个样本转换为模型的输入格式，然后使用这个函数处理整个数据集。\n",
    "\"\"\"\n",
    "\n",
    "# 定义一个名为 tokenize_function 的函数，该函数接受一个样本作为输入，然后返回处理后的样本。\n",
    "def tokenize_function(example):\n",
    "    # 构造模型的输入提示。输入提示包括对话的内容和一个摘要的开头，后面的模型将生成摘要的剩余部分。\n",
    "    prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    inp = [prompt + i + end_prompt for i in example[\"dialogue\"]]\n",
    "\n",
    "    # 使用预训练的分词器将输入提示转换为模型的输入格式，并保存到 'input_ids' 字段。\n",
    "    # inp是需要被处理的输入文本。\n",
    "    # padding=\"max_length\"：如果输入的文本长度小于tokenizer的最大长度，那么它将添加特定的填充符号以达到最大长度。这样可以确保所有的输入都有相同的长度，以便能够在神经网络中进行处理。\n",
    "    # truncation=True：如果输入的文本长度大于tokenizer的最大长度，那么它将截断文本以适应最大长度。\n",
    "    # return_tensors=\"pt\"：这表示返回的数据应该是PyTorch张量。这对于将数据输入到PyTorch模型中是必要的。\n",
    "    # .input_ids：这是从tokenizer返回的数据中提取'input_ids'的方法。'input_ids'是一个包含每个输入符号的数字表示的列表。\n",
    "    # 这个数字表示是通过预先定义的词汇表来确定的，每个数字对应词汇表中的一个单词或符号。\n",
    "    example['input_ids'] = tokenizer(inp, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # 使用预训练的分词器将摘要转换为模型的输入格式，并保存到 'labels' 字段。\n",
    "    # 设置 padding=\"max_length\" 和 truncation=True 表示如果输入的长度超过模型的最大长度，那么将其截断到最大长度。\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # 返回处理后的样本。\n",
    "    return example\n",
    "\n",
    "# 使用 map 函数和 tokenize_function 函数处理整个数据集。设置 batched=True 表示对数据集的一批样本进行并行处理。\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 使用 remove_columns 函数移除不需要的列。这些列包括 'id'、'topic'、'dialogue' 和 'summary'。\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详细地分析一下批处理过程中的 tokenize_function 函数。为了简化讨论，假设我们的批次大小是2，也就是我们一次处理两个样本。\n",
    "\n",
    "**步骤1：接收批次数据**\n",
    "\n",
    "首先，函数 tokenize_function 会接收一个包含批次数据的字典，该字典的键是特征名称（例如，\"dialogue\"和\"summary\"），值是一个列表，列表的长度等于批次大小。例如，这个字典可能如下所示：\n",
    "\n",
    "```\n",
    "example = {\n",
    "    \"dialogue\": [\n",
    "        \"Person 1: Hi, how are you?\\nPerson 2: I'm good, thanks. How about you?\",\n",
    "        \"Person 1: What's the weather like today?\\nPerson 2: It's sunny and warm.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Person 1 and Person 2 greeted each other and asked about each other's well-being.\",\n",
    "        \"Person 1 asked about the weather and Person 2 responded it's sunny and warm.\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**步骤2：构建模型的输入提示**\n",
    "\n",
    "然后，该函数会为每个对话创建一个输入提示，该提示包含对话的内容和一个摘要的开头。这个过程是通过列表推导完成的：\n",
    "```\n",
    "prompt = 'Summarize the following conversation.\\n\\n'\n",
    "end_prompt = '\\n\\nSummary: '\n",
    "inp = [prompt + i + end_prompt for i in example[\"dialogue\"]]\n",
    "```\n",
    "\n",
    "我们得到的 inp 列表如下：\n",
    "```\n",
    "[\n",
    "    \"Summarize the following conversation.\\n\\nPerson 1: Hi, how are you?\\nPerson 2: I'm good, thanks. How about you?\\n\\nSummary: \",\n",
    "    \"Summarize the following conversation.\\n\\nPerson 1: What's the weather like today?\\nPerson 2: It's sunny and warm.\\n\\nSummary: \"\n",
    "]\n",
    "```\n",
    "\n",
    "**步骤3：使用分词器处理输入提示和摘要**\n",
    "\n",
    "接下来，我们使用分词器将输入提示和摘要转换为模型的输入格式：\n",
    "```\n",
    "example['input_ids'] = tokenizer(inp, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "```\n",
    "这两行代码会将输入提示和摘要分别转换为模型的输入 ID，并添加到 example 字典中。分词器会处理文本的长度，如果文本的长度超过模型的最大长度，那么将其截断到最大长度。\n",
    "\n",
    "**步骤4：返回处理后的批次数据**\n",
    "\n",
    "最后，函数返回处理后的批次数据：\n",
    "```\n",
    "return example\n",
    "```\n",
    "这个 example 字典可能如下所示：\n",
    "```\n",
    "{\n",
    "    'dialogue': [\n",
    "        \"Person 1: Hi, how are you?\\nPerson 2: I'm good, thanks. How about you?\",\n",
    "        \"Person 1: What's the weather like today?\\nPerson 2: It's sunny and warm.\"\n",
    "    ],\n",
    "    'summary': [\n",
    "        \"Person 1 and Person 2 greeted each other and asked about each other's well-being.\",\n",
    "        \"Person 1 asked about the weather and Person 2 responded it's sunny and warm.\"\n",
    "    ],\n",
    "    'input_ids': tensor([[  101,  2595,  4931,  ...,     0,     0,     0],\n",
    "                         [  101,  2595,  4931,  ...,     0,     0,     0]]),\n",
    "    'labels': tensor([[ 101, 2023, 2003, ...,    0,    0,    0],\n",
    "                      [ 101, 2023, 2003, ...,    0,    0,    0]])\n",
    "}\n",
    "```\n",
    "在这个字典中，'input_ids' 和 'labels' 是两个二维张量，第一维度是批次大小（在这个例子中，批次大小为2），第二维度是序列长度。值是单词在词典中的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将预处理步骤封装成一个可重复使用的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n定义了两个函数：tokenize_function 和 transform_dataset，它们的主要作用是对输入的数据集进行预处理。\\n\\ntokenize_function 是一个对数据集样本进行标记（tokenization）的函数。它接收一个样本作为输入，然后加上特定的提示和结束提示，将对话和摘要信息转换为模型可以理解的标记形式。在这个过程中，对话部分被存储在 'input_ids' 中，摘要部分被存储在 'labels' 中。\\n\\ntransform_dataset 则是一个对数据集进行预处理的函数，包括划分数据集和标记。首先，它从指定路径加载原始数据集，然后加载适用于特定模型的标记器。接着，它将数据集划分为训练集、测试集和验证集。之后，对这三个数据集进行标记，将结果存储在新的数据集字典（tokenized_datasets）中。最后，将这个数据集字典分别写入到训练、测试和验证的文件夹中。\\n\\n这两个函数的组合使用，可以完成从原始数据集到模型可以直接使用的预处理数据集的转换，包括数据集的划分、样本的标记以及预处理数据的存储。\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个函数用于对数据集样本进行标记（tokenization）\n",
    "def tokenize_function(example):\n",
    "    # 定义输入的提示语\n",
    "    prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    # 定义输入的结束提示语\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    # 对每个对话进行处理，加上提示和结束提示\n",
    "    inp = [prompt + i + end_prompt for i in example[\"dialogue\"]]\n",
    "    # 对处理后的对话进行标记，将结果存入'input_ids'\n",
    "    example['input_ids'] = tokenizer(inp, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # 对摘要进行标记，将结果存入'labels'\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    # 返回处理后的样本\n",
    "    return example\n",
    "\n",
    "# 定义一个函数用于对数据集进行预处理（包括划分数据集和标记）\n",
    "def transform_dataset(input_data,\n",
    "                      output_data,\n",
    "                      huggingface_model_name,\n",
    "                      train_split_percentage,\n",
    "                      test_split_percentage,\n",
    "                      validation_split_percentage,\n",
    "                      ):\n",
    "\n",
    "    # 加载原始数据集\n",
    "    dataset = load_dataset(input_data)\n",
    "    print(f'Dataset loaded from path: {input_data}\\n{dataset}')\n",
    "    \n",
    "    # 加载标记器\n",
    "    print(f'Loading the tokenizer for the model {huggingface_model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name)\n",
    "    \n",
    "    # 划分训练集、测试集和验证集\n",
    "    train_testvalid = dataset['train'].train_test_split(1 - train_split_percentage, seed=1234)\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_split_percentage / (validation_split_percentage + test_split_percentage), seed=1234)\n",
    "    # 创建一个数据集字典（DatasetDict），其中包括训练集、测试集和验证集。\n",
    "    # DatasetDict 是 Hugging Face 的 datasets 库中的一个类，它可以用于存储和操作多个数据集。\n",
    "    # 这里创建的 train_test_valid_dataset 就包括了训练集、测试集和验证集，使得这三个数据集可以被统一管理。\n",
    "    train_test_valid_dataset = DatasetDict(\n",
    "        {\n",
    "            # 训练集\n",
    "            'train': train_testvalid['train'],\n",
    "            # 测试集\n",
    "            'test': test_valid['test'],\n",
    "            # 验证集\n",
    "            'validation': test_valid['train']\n",
    "        }\n",
    "    )\n",
    "    print(f'Dataset after splitting:\\n{train_test_valid_dataset}')\n",
    "    \n",
    "    # 对数据集进行标记\n",
    "    print(f'Tokenizing the dataset...')\n",
    "    tokenized_datasets = train_test_valid_dataset.map(tokenize_function, batched=True)\n",
    "    # 移除原始数据集的某些列\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
    "    print(f'Tokenizing complete!')\n",
    "    \n",
    "    # 创建输出文件夹\n",
    "    os.makedirs(f'{output_data}/train/', exist_ok=True)\n",
    "    os.makedirs(f'{output_data}/test/', exist_ok=True)\n",
    "    os.makedirs(f'{output_data}/validation/', exist_ok=True)\n",
    "    file_root = str(int(time.time()*1000))\n",
    "    \n",
    "    # 将预处理后的数据集写入磁盘\n",
    "    print(f'Writing the dataset to {output_data}')\n",
    "    tokenized_datasets['train'].to_parquet(f'./{output_data}/train/{file_root}.parquet')\n",
    "    tokenized_datasets['test'].to_parquet(f'./{output_data}/test/{file_root}.parquet')\n",
    "    tokenized_datasets['validation'].to_parquet(f'./{output_data}/validation/{file_root}.parquet')\n",
    "    print('Preprocessing complete!')\n",
    "    \n",
    "'''\n",
    "定义了两个函数：tokenize_function 和 transform_dataset，它们的主要作用是对输入的数据集进行预处理。\n",
    "\n",
    "tokenize_function 是一个对数据集样本进行标记（tokenization）的函数。\n",
    "它接收一个样本作为输入，然后加上特定的提示和结束提示，将对话和摘要信息转换为模型可以理解的标记形式。在这个过程中，对话部分被存储在 'input_ids' 中，摘要部分被存储在 'labels' 中。\n",
    "\n",
    "transform_dataset 则是一个对数据集进行预处理的函数，包括划分数据集和标记。\n",
    "首先，它从指定路径加载原始数据集，然后加载适用于特定模型的标记器。\n",
    "接着，它将数据集划分为训练集、测试集和验证集。之后，对这三个数据集进行标记，将结果存储在新的数据集字典（tokenized_datasets）中。\n",
    "最后，将这个数据集字典分别写入到训练、测试和验证的文件夹中。\n",
    "\n",
    "这两个函数的组合使用，可以完成从原始数据集到模型可以直接使用的预处理数据集的转换，包括数据集的划分、样本的标记以及预处理数据的存储。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "定义函数 process，该函数主要用于处理指定的输入数据，并将结果存储到指定的输出目录。\n",
    "'''\n",
    "\n",
    "def process(args):\n",
    "    # 打印输入数据目录的路径\n",
    "    print(f\"Listing contents of {args.input_data}\")\n",
    "    # 列出输入数据目录下的所有文件\n",
    "    dirs_input = os.listdir(args.input_data)\n",
    "    for file in dirs_input:\n",
    "        print(file)\n",
    "\n",
    "    # 调用transform_dataset函数对数据进行预处理\n",
    "    # 包括加载数据、划分数据集、对数据进行标记化、存储处理后的数据\n",
    "    transform_dataset(input_data=args.input_data,  # 输入数据的路径\n",
    "                      output_data=args.output_data,  # 输出数据的路径\n",
    "                      huggingface_model_name=args.model_checkpoint,  # 使用的预训练模型的名字\n",
    "                      train_split_percentage=args.train_split_percentage,  # 训练集的比例\n",
    "                      test_split_percentage=args.test_split_percentage,  # 测试集的比例\n",
    "                      validation_split_percentage=args.validation_split_percentage,  # 验证集的比例\n",
    "                     )\n",
    "\n",
    "    # 打印输出数据目录的路径\n",
    "    print(f\"Listing contents of {args.output_data}\")\n",
    "    # 列出输出数据目录下的所有文件\n",
    "    dirs_output = os.listdir(args.output_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在本地处理自然语言处理（NLP）数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/data-summarization-55f5a7efa2dfb458/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing contents of ./data-summarization\n",
      "dialogsum-2.csv\n",
      "dialogsum-1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41bfb73d39046bab35f01512e4fde08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from path: ./data-summarization\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 14460\n",
      "    })\n",
      "})\n",
      "Loading the tokenizer for the model google/flan-t5-base\n",
      "Dataset after splitting:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 13014\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 723\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 723\n",
      "    })\n",
      "})\n",
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing complete!\n",
      "Writing the dataset to ./data-summarization-processed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe990dd7c1b40d5bb52d26242d31e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5226fc5da82431f842f8f1ec6d718a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1a55bfcf0b4b3994a491cf158e19f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "Listing contents of ./data-summarization-processed\n",
      "test\n",
      "train\n",
      "validation\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    input_data: str  # 输入数据的路径\n",
    "    output_data: str  # 输出数据的路径\n",
    "    model_checkpoint: str  # 使用的预训练模型的名字\n",
    "    train_split_percentage: float  # 训练集的比例\n",
    "    test_split_percentage: float  # 测试集的比例\n",
    "    validation_split_percentage: float  # 验证集的比例\n",
    "\n",
    "args = Args()  # 创建Args类的实例\n",
    "\n",
    "args.model_checkpoint = model_checkpoint  # 设置模型名字\n",
    "args.input_data = './data-summarization'  # 设置输入数据的路径\n",
    "args.output_data = './data-summarization-processed'  # 设置输出数据的路径\n",
    "args.train_split_percentage = 0.9  # 设置训练集的比例\n",
    "args.test_split_percentage = 0.05  # 设置测试集的比例\n",
    "args.validation_split_percentage = 0.05  # 设置验证集的比例\n",
    "\n",
    "# 如果输出数据的目录已经存在，就删除该目录\n",
    "if os.path.isdir(args.output_data):\n",
    "    import shutil\n",
    "    shutil.rmtree(args.output_data)\n",
    "\n",
    "# 调用 process 函数对输入数据进行预处理。\n",
    "process(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 确保数据集能够正确加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/data-summarization-processed to /root/.cache/huggingface/datasets/parquet/data-summarization-processed-501c81c8367fa59b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d2312289bc413fbfe0d80cb2ff6fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4b7be02c0a4d7083c3886a408a3025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/data-summarization-processed-501c81c8367fa59b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60ce6d44f1f4cb7bf18277f52868790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "使用 load_dataset 函数从指定的路径加载数据集。这里加载的数据集是在前面的预处理步骤中生成的，并已经存储到了指定的目录下。\n",
    "'''\n",
    "\n",
    "# 使用load_dataset函数加载数据集\n",
    "dataset = load_dataset(\n",
    "    './data-summarization-processed/',  # 数据集所在的路径\n",
    "    data_files={'train': 'train/*.parquet',  # 训练集文件的路径，这里使用了通配符，表示加载该目录下的所有parquet文件\n",
    "                'test': 'test/*.parquet',  # 测试集文件的路径\n",
    "                'validation': 'validation/*.parquet'}  # 验证集文件的路径\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存储变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_data_processed_path = './data-summarization-processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'local_data_processed_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store local_data_processed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "ingest_create_athena_table_parquet_passed             -> True\n",
      "local_data_processed_path                             -> './data-summarization-processed/'\n",
      "model_checkpoint                                      -> 'google/flan-t5-base'\n",
      "raw_input_data_s3_uri                                 -> 's3://sagemaker-us-east-1-941797585610/data-summar\n",
      "role                                                  -> 'arn:aws:iam::941797585610:role/service-role/Amazo\n",
      "s3_private_path_tsv                                   -> 's3://sagemaker-us-east-1-941797585610/amazon-revi\n",
      "s3_public_path_tsv                                    -> 's3://dsoaws/tsv'\n",
      "setup_dependencies_passed                             -> True\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
