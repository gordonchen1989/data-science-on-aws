{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PEFT 微调用于对话摘要的生成式 AI 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"img/kernel_set_up.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"img/w3_kernel_and_instance_type.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入datasets库的load_dataset函数，用于加载数据集。\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 导入transformers库的一些模型和功能。AutoModelForSeq2SeqLM用于加载序列到序列的模型，AutoTokenizer用于加载tokenizer，\n",
    "# GenerationConfig用于配置模型生成的参数，TrainingArguments用于配置训练参数，Trainer用于训练模型。\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "\n",
    "# 导入torch库，是一个用于深度学习的开源库。\n",
    "import torch\n",
    "\n",
    "# 导入time库，用于处理时间相关的操作。\n",
    "import time\n",
    "\n",
    "# 导入evaluate模块，通常用于评估模型的性能（这个模块的具体内容会根据你的代码库和项目有所不同）。\n",
    "import evaluate\n",
    "\n",
    "# 导入pandas库，这是一个用于数据处理和分析的库。\n",
    "import pandas as pd\n",
    "\n",
    "# 导入numpy库，这是一个用于数值计算的库。\n",
    "import numpy as np\n",
    "\n",
    "# 导入tqdm库，这是一个快速，可扩展的Python进度条，可以在长循环中添加一个进度提示信息，用户只需要封装任意的迭代器tqdm(iterator)。\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tqdm库的progress_apply方法可以方便地将进度条应用到Pandas的DataFrame或Series的apply方法上。\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 加载数据集和 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-1d0df498900a79f1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70994e8014247b492d028b7432ba076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从Hugging Face的数据集库中加载一个名为\"knkarthick/dialogsum\"的数据集，并将其保存到变量dataset中。\n",
    "\n",
    "# 设置Hugging Face数据集库中的数据集名称。\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "# 使用load_dataset函数加载指定名称的数据集。\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "# 显示加载的数据集。\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 从Hugging Face的模型库中加载一个预训练的序列到序列模型和对应的tokenizer。这里选择的模型是谷歌的'flan-t5-base'模型。\n",
    "# AutoModelForSeq2SeqLM.from_pretrained函数会根据提供的名称从Hugging Face的模型库中查找并下载对应的预训练模型。\n",
    "# 这个模型是一个序列到序列（Seq2Seq）模型，适用于像机器翻译、文本摘要等任务。\n",
    "# AutoTokenizer.from_pretrained函数同样会从模型库中查找并下载对应的tokenizer。\n",
    "# Tokenizer是用于将原始文本转化为模型可以处理的格式的工具。\n",
    "\n",
    "# 设置Hugging Face模型库中的模型名称。\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "# 使用AutoModelForSeq2SeqLM的from_pretrained方法加载预训练模型。torch_dtype=torch.bfloat16表示模型的张量数据类型为bfloat16，\n",
    "# 这种数据类型可以在保持大部分精度的同时，减少模型的内存占用，加速计算。\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# 使用AutoTokenizer的from_pretrained方法加载与预训练模型相对应的tokenizer，用于将原始文本转化为模型可以处理的格式。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# 这段代码定义了一个函数print_number_of_trainable_model_parameters，用于打印给定模型的可训练参数数量、总参数数量以及可训练参数占总参数的百分比。\n",
    "\n",
    "# 定义函数，接受一个模型作为输入。\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    # 初始化可训练参数和总参数的计数器。\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    # 遍历模型的所有参数。\n",
    "    for _, param in model.named_parameters():\n",
    "        # 计算所有参数的总数。numel()函数返回参数中元素的数量。\n",
    "        all_model_params += param.numel()\n",
    "\n",
    "        # 如果参数需要梯度（即可训练），则计算可训练参数的数量。\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "\n",
    "    # 返回一个格式化的字符串，显示可训练参数数量、总参数数量和可训练参数占总参数的百分比。\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params}%\"\n",
    "\n",
    "# 调用函数，打印模型的参数信息。\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 使用Zero Shot 推理测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的功能是从测试数据集中选择一条对话数据，使用预训练的模型生成对应的对话摘要，并将人工编写的摘要（基线）和模型生成的摘要进行比较。\n",
    "\n",
    "# 选择测试数据集中的一个索引。\n",
    "index = 200\n",
    "\n",
    "# 从测试数据集中获取对话和摘要。\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "# 构建模型输入的提示，包括要求模型进行对话摘要的指示和实际的对话内容。\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 使用tokenizer将输入提示转换为模型可以处理的形式，返回张量格式的输入。\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# 使用模型生成摘要。generate函数接受输入ID和最大新生成令牌数（这里设为200个），然后返回生成的摘要的ID。\n",
    "# decode函数将生成摘要的ID转换回文本形式，并跳过特殊令牌（比如开始和结束令牌）。\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# 定义一个分隔线，用于在打印输出时分隔不同部分的内容。\n",
    "dash_line = ('-'.join('' for x in range(100)))\n",
    "\n",
    "# 打印输入的提示、基线人工摘要和模型生成的摘要。\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 执行全面微调 Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-1d0df498900a79f1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-5f46830011c7eba2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-1d0df498900a79f1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-6aff51518f8c17a6.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 这段代码首先定义了一个函数tokenize_function，该函数将原始对话和摘要文本转换为模型训练所需的格式，然后对数据集应用这个函数，最后移除了数据集中不需要的列。\n",
    "\n",
    "# 定义函数，接受一个example（包含对话和摘要的字典）作为输入。\n",
    "def tokenize_function(example):\n",
    "    # 定义模型输入的提示，包括开始和结束的提示语。\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    # 将对话和提示语合并，生成模型的输入。\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "\n",
    "    # 使用tokenizer将输入提示转换为模型可以处理的形式，返回张量格式的输入。\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # 使用tokenizer将摘要转换为模型可以处理的形式，作为训练目标（标签）。\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # 返回处理后的example。\n",
    "    return example\n",
    "\n",
    "# 使用map函数将tokenize_function应用到数据集的每一个元素，参数batched=True表示按批次处理数据。\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 使用remove_columns函数移除数据集中不需要的列，包括'id'、'topic'、'dialogue'和'summary'。\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-1d0df498900a79f1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-dc63ff0c1c2e5347.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-1d0df498900a79f1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-54224b7cd734d868.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用filter函数筛选出数据集中索引为100的倍数的样本。\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 打印训练、验证和测试数据集的形状，即样本数量。\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "# 打印数据集的详细信息，包括数据集的名称、版本、描述、许可证、格式等。\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 设置输出目录的名称，其中包含了当前的时间戳，以区分不同的训练运行。\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# 初始化TrainingArguments实例，定义了训练过程中的一些参数。\n",
    "training_args = TrainingArguments(\n",
    "    # 指定模型和训练日志的保存路径。\n",
    "    output_dir=output_dir,\n",
    "    # 设置学习率，决定了模型参数在训练中的更新速度。\n",
    "    learning_rate=1e-5,\n",
    "    # 设置训练的轮数，每一轮会遍历整个训练数据集一次。\n",
    "    num_train_epochs=1,\n",
    "    # 设置权重衰减，用于正则化模型，防止过拟合。\n",
    "    weight_decay=0.01,\n",
    "    # 指定每隔多少步打印训练日志。\n",
    "    logging_steps=1,\n",
    "    # 指定训练的最大步数，如果设为1，那么训练将在完成一步后立即结束。\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "# 初始化Trainer实例，用于进行模型训练。\n",
    "trainer = Trainer(\n",
    "    # 指定要训练的模型。\n",
    "    model=original_model,\n",
    "    # 指定训练参数。\n",
    "    args=training_args,\n",
    "    # 指定训练数据集。\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    # 指定验证数据集，用于在训练过程中评估模型的性能。\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=49.5, metrics={'train_runtime': 3.2011, 'train_samples_per_second': 2.499, 'train_steps_per_second': 0.312, 'total_flos': 5478058819584.0, 'train_loss': 49.5, 'epoch': 0.06})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/generation_config.json to flan-dialogue-summary-checkpoint/generation_config.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/training_args.bin to flan-dialogue-summary-checkpoint/training_args.bin\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/config.json to flan-dialogue-summary-checkpoint/config.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/rng_state.pth to flan-dialogue-summary-checkpoint/rng_state.pth\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/scheduler.pt to flan-dialogue-summary-checkpoint/scheduler.pt\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/trainer_state.json to flan-dialogue-summary-checkpoint/trainer_state.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/pytorch_model.bin to flan-dialogue-summary-checkpoint/pytorch_model.bin\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/optimizer.pt to flan-dialogue-summary-checkpoint/optimizer.pt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dsoaws/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 990408885 Apr 10 17:51 ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行定性评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 获取GPU的数量\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# 获取当前设备的索引，如果你在使用GPU，这将返回你正在使用的GPU的索引\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "# 获取当前设备的名称\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 判断设备是否有GPU资源\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "# 从指定路径加载预训练的序列到序列的语言模型，并指定模型的数据类型为bfloat16\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16).to(device)\n",
    "# torch_dtype=torch.bfloat16是一个指定模型数据类型的参数。\n",
    "# bfloat16是一种16位宽的浮点数数据类型，相比32位的float32数据类型，它可以减少模型的内存占用和计算时间，但可能会略微降低模型的精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的主要目的是比较两个模型：original_model和instruct_model在对同一对话进行摘要的性能。同时，也与人类的base line摘要进行了比较。\n",
    "\n",
    "# 选择测试数据集中的一个样本的索引\n",
    "index = 200\n",
    "# 提取此索引对应的对话\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "# 提取此索引对应的人类基线摘要\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# 构造模型的输入提示，包含对话和需要模型进行的任务\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 使用tokenizer将输入提示转化为模型可以接受的输入形式\n",
    "# 准备输入数据时，确保与模型在同一设备上\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# 使用original_model生成对话摘要\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# 将模型生成的摘要从id形式转化为文本形式\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 使用instruct_model生成对话摘要\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# 将模型生成的摘要从id形式转化为文本形式\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印出人类基线摘要、original_model生成的摘要和instruct_model生成的摘要\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行定量评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')  # 加载'rouge'评价指标\n",
    "# ROUGE (Recall-Oriented Understudy for Gisting Evaluation)是一种评估自动文本摘要的方法，尤其在机器翻译和自动摘要生成等任务中使用较多。\n",
    "# ROUGE主要通过比较机器生成的摘要和人类编写的参考摘要之间的重叠来评价机器生成的摘要的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成测试数据集样本的输出（为了节省时间，只有 10 个对话和摘要），然后保存结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection is a problem.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection is a problem.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection is a problem.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I'm so happy you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time.</td>\n",
       "      <td>Brian's birthday is coming. #Person1# invites Brian to have a dance and Brian compliments #Person1#'s looks. Brian thinks #Person1# looks great and invites #Person1# to have a drink together.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                          human_baseline_summaries  \\\n",
       "0                                              Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n",
       "1  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n",
       "2                                  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.   \n",
       "3                                                       #Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.   \n",
       "4                                                                                      #Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.   \n",
       "5                                                                            #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.   \n",
       "6                                                                                            #Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.   \n",
       "7                                                                                         #Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.   \n",
       "8                                                                                 #Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched   \n",
       "9                                                                                                       #Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                 original_model_summaries  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                          #Person1#: I need to take a dictation for you.   \n",
       "1                                                                                                                                                                                                                                                                                                                                          #Person1#: I need to take a dictation for you.   \n",
       "2                                                                                                                                                                                                                                                                                                                                          #Person1#: I need to take a dictation for you.   \n",
       "3                                                                                                                                                                                                                                                                                                                             The traffic jam at the Carrefour intersection is a problem.   \n",
       "4                                                                                                                                                                                                                                                                                                                             The traffic jam at the Carrefour intersection is a problem.   \n",
       "5                                                                                                                                                                                                                                                                                                                             The traffic jam at the Carrefour intersection is a problem.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                    Masha and Hero are getting divorced.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                    Masha and Hero are getting divorced.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                    Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I'm so happy you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                    instruct_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.  \n",
       "3                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.  \n",
       "4                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.  \n",
       "5                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.  \n",
       "6                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.  \n",
       "7                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.  \n",
       "8                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.  \n",
       "9                                                                                                                                                                                                                                            Brian's birthday is coming. #Person1# invites Brian to have a dance and Brian compliments #Person1#'s looks. Brian thinks #Person1# looks great and invites #Person1# to have a drink together.  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这段代码的主要目标是对测试数据集中的一些对话进行摘要，并将人类编写的摘要与两种不同模型生成的摘要进行比较。\n",
    "\n",
    "# 从数据集中获取测试数据的前10条对话和对应的人类编写的摘要\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# 初始化列表，用于保存两种模型生成的摘要\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "# 对测试数据中的每一条对话进行迭代\n",
    "for _, dialogue in enumerate(tqdm(dialogues)):\n",
    "    # 定义模型的输入提示\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    # 使用tokenizer处理输入提示，并获取输入的id\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # 使用original_model生成对话摘要\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    # 将模型生成的摘要从id形式转化为文本形式\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    # 将生成的摘要添加到original_model_summaries列表中\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    # 使用instruct_model生成对话摘要\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    # 将模型生成的摘要从id形式转化为文本形式\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    # 将生成的摘要添加到instruct_model_summaries列表中\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "# 将人类编写的摘要、original_model生成的摘要和instruct_model生成的摘要组合在一起\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "# 设置pandas显示列的最大宽度\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "# 创建一个pandas的DataFrame，用于显示每一条对话的人类编写的摘要、original_model生成的摘要和instruct_model生成的摘要\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df  # 显示这个DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23884559093833285, 'rouge2': 0.11535720375106562, 'rougeL': 0.21714203657752046, 'rougeLsum': 0.2175800707655546}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的主要目标是计算并打印两种模型生成的摘要与人类编写的摘要之间的ROUGE得分。ROUGE得分是用来评估自动文本摘要或机器翻译的质量的一种指标。\n",
    "\n",
    "# 使用ROUGE指标计算original_model生成的摘要和人类编写的摘要之间的相似度\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,  # original_model生成的摘要\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],  # 对应的人类编写的摘要\n",
    "    use_aggregator=True,  # 使用聚合器，用于计算所有摘要的平均ROUGE得分\n",
    "    use_stemmer=True,  # 使用词干提取，将单词变为其基本形式以进行比较\n",
    ")\n",
    "\n",
    "# 使用ROUGE指标计算instruct_model生成的摘要和人类编写的摘要之间的相似度\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,  # instruct_model生成的摘要\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],  # 对应的人类编写的摘要\n",
    "    use_aggregator=True,  # 使用聚合器，用于计算所有摘要的平均ROUGE得分\n",
    "    use_stemmer=True,  # 使用词干提取，将单词变为其基本形式以进行比较\n",
    ")\n",
    "\n",
    "# 打印original_model的ROUGE得分\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "# 打印instruct_model的ROUGE得分\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"
     ]
    }
   ],
   "source": [
    "# 引入pandas库，并读取CSV文件的数据，将其存储在results变量中\n",
    "results = pd.read_csv(\"data-peft/dialogue-summary-training-results-peft.csv\")\n",
    "\n",
    "# 从results数据框架中获取'human_baseline_summaries'列的值，并将其转换为numpy数组，存储在human_baseline_summaries变量中\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "\n",
    "# 从results数据框架中获取'original_model_summaries'列的值，并将其转换为numpy数组，存储在original_model_summaries变量中\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "\n",
    "# 从results数据框架中获取'instruct_model_summaries'列的值，并将其转换为numpy数组，存储在instruct_model_summaries变量中\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "# 使用ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 度量方法计算原始模型的预测摘要和人类基线摘要之间的相似度\n",
    "# 使用聚合器和词干提取器进行计算，结果存储在original_model_results变量中\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# 使用ROUGE度量方法计算指导模型的预测摘要和人类基线摘要之间的相似度\n",
    "# 使用聚合器和词干提取器进行计算，结果存储在instruct_model_results变量中\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# 打印原始模型的评估结果\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "# 打印指导模型的评估结果\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 18.82%\n",
      "rouge2: 10.43%\n",
      "rougeL: 13.70%\n",
      "rougeLsum: 13.69%\n"
     ]
    }
   ],
   "source": [
    "# 打印一条消息，表示我们将计算指导模型相对于人类基线模型的百分比改进情况\n",
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "# 计算指导模型的评估结果和原始模型的评估结果的差异，并将其转换为numpy数组\n",
    "# 这个差异就是指导模型相对于原始模型的改进程度\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "\n",
    "# 遍历指导模型的评估结果的每一个键（即评估指标名称）和对应的改进程度\n",
    "# 将每一个改进程度乘以100，转换为百分比，并保留两位小数\n",
    "# 打印每一个评估指标的名称和对应的百分比改进情况\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/peft_lora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora的例子：\n",
    "\n",
    "假设你有一本非常厚的书，这本书包含了许多的信息。然而，你需要在一个小书包中带着这本书。显然，这本书太大，无法放入书包。这个时候，Lora就像是一个压缩工具，它可以将这本书压缩成一本小册子，这本小册子虽然比原书小，但它包含了原书的主要信息。\n",
    "\n",
    "当你需要在外面查阅书中的信息时，你不再需要带着那本厚重的书，而是可以带着那本压缩后的小册子。这样，你不仅可以节省空间，还可以节省查阅信息所需的时间。\n",
    "\n",
    "这就是Lora的主要思想。在语言模型中，\"厚重的书\"就是原始的Attention矩阵，\"小册子\"就是Lora参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用一个更形象的比喻来解释Lora参数是如何捕获原始矩阵的主要信息的。\n",
    "\n",
    "想象一下，你正在看一部电影。这部电影很长，有很多细节和复杂的情节。但是，你只有几分钟的时间，你想了解电影的大致内容。\n",
    "\n",
    "这时，你可能会选择看电影的预告片。预告片是电影的一个简化版本，它虽然没有电影的所有细节，但是它可以在短时间内给你一个关于电影的大体概念。\n",
    "\n",
    "在这个比喻中，电影就像是我们的原始矩阵，预告片就像是我们的Lora参数。Lora参数通过保留最重要的特征（就像预告片保留电影的关键情节）来简化原始矩阵，从而使我们可以用更少的资源（就像看预告片比看整部电影更节约时间）来理解和使用模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看一个更具体的例子来说明Lora在模型微调中的应用。\n",
    "\n",
    "假设我们有一个预训练的语言模型，它已经被训练来理解和生成自然语言文本。我们希望微调这个模型来执行一个特殊的任务，比如情感分析，即预测输入文本的情感是积极还是消极。\n",
    "\n",
    "在原始模型（没有应用Lora）中，我们需要调整模型中的所有参数来适应新的任务。这个过程可能会非常耗费计算资源，并且有可能导致过拟合，因为模型可能会过度地适应新的训练数据。\n",
    "\n",
    "然而，如果我们应用Lora，我们可以将模型的注意力参数从一个高维空间（原始参数空间）映射到一个低维空间（Lora参数空间）。这样，我们只需要微调这个低维空间中的参数，而不是原始的高维空间中的所有参数。\n",
    "\n",
    "例如，假设原始的注意力参数是一个10000x10000的矩阵，我们可以通过Lora将其映射到一个10x10的矩阵。然后，我们只需要微调这个10x10的矩阵，而不是原始的10000x10000的矩阵。这将大大减少微调过程中所需的计算资源，并且有可能提高模型在新任务上的性能，因为低维空间中的参数可以更好地捕获原始矩阵的主要信息。\n",
    "\n",
    "总的来说，Lora在模型微调中的应用就是通过将模型的参数映射到一个低维空间，然后在这个低维空间中进行微调，从而提高模型的效率和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 引入需要的模块和类\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 创建一个LoraConfig对象，用于配置LORA（Low-Rank Adaptation）模型的参数\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank，表示低秩适配的秩，即在低秩空间中的维度\n",
    "    lora_alpha=32, # alpha参数，表示在训练过程中对LORA层的缩放因子\n",
    "    target_modules=[\"q\", \"v\"], # target_modules参数，表示要应用LORA的模块\n",
    "    lora_dropout=0.05, # dropout参数，表示在LORA层中的dropout比例\n",
    "    bias=\"none\", # bias参数，表示是否在LORA层中添加偏置项\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # task_type参数，表示任务类型，这里是序列到序列的语言模型任务\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r`：这是低秩适配的秩，表示在低秩空间中的维度。这个值越大，低维空间的维度就越高，可以捕获更多的原始信息，但计算和存储需求也会更高。相反，如果这个值越小，低维空间的维度就越低，虽然计算和存储需求会降低，但可能会丢失一些原始信息。\n",
    "\n",
    "`lora_alpha`：这是一个缩放因子，用于在训练过程中调整Lora层的学习率。这个值越大，Lora层的学习率就越高，模型可能会更快地适应新的任务，但也可能会导致过拟合。相反，如果这个值越小，Lora层的学习率就越低，过拟合的风险可能会降低，但模型可能需要更长的时间来适应新的任务。\n",
    "\n",
    "`target_modules`：这是一个列表，定义了要应用Lora的模块。在这个例子中，我们选择了\"q\"和\"v\"，这表示我们将Lora应用到注意力机制的查询向量（\"q\"）和值向量（\"v\"）。\n",
    "\n",
    "`lora_dropout`：这是在Lora层中的dropout比例，用于防止过拟合。在训练过程中，每个步骤中都会随机地将一部分Lora层的神经元设为0，这可以帮助模型更好地泛化到未见过的数据。\n",
    "\n",
    "`bias`：这个参数表示是否在Lora层中添加偏置项。在这个例子中，我们选择了\"none\"，表示我们不在Lora层中添加偏置项。\n",
    "\n",
    "`task_type`：这个参数表示任务类型。在这个例子中，我们选择了TaskType.SEQ_2_SEQ_LM，表示我们的任务是一个序列到序列的语言模型任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.4092820552029972%\n"
     ]
    }
   ],
   "source": [
    "# 使用原始模型和LORA配置信息，获取带有PEFT（Performance-Enhancing Fine-Tuning）的模型\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "# 打印出PEFT模型的可训练参数的数量\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这段代码的主要目的是设置PEFT模型的训练参数，并创建一个用于训练模型的训练器。\n",
    "\n",
    "# 使用当前时间戳创建一个唯一的输出目录，用于保存训练过程中的模型和日志文件\n",
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# 创建一个TrainingArguments对象，该对象包含了训练参数\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # 指定输出目录\n",
    "    auto_find_batch_size=True,  # 自动寻找合适的批次大小\n",
    "    learning_rate=1e-3,  # 设置学习率，这里的学习率比全模型微调的学习率要高\n",
    "    num_train_epochs=1,  # 设置训练轮次为1\n",
    "    logging_steps=1,  # 每训练1步就记录日志\n",
    "    max_steps=1  # 最大训练步数为1\n",
    ")\n",
    "\n",
    "# 创建一个Trainer对象，用于训练模型\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,  # 指定要训练的模型\n",
    "    args=peft_training_args,  # 指定训练参数\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # 指定训练数据集\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这段代码的主要目的是开始训练模型并保存训练后的模型和对应的分词器。\n",
    "\n",
    "# 使用Trainer对象开始训练模型\n",
    "peft_trainer.train()\n",
    "\n",
    "# 指定保存模型的路径\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "# 保存训练后的模型到指定路径\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "\n",
    "# 保存模型使用的分词器到指定路径\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsoaws/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n",
      "download: s3://dsoaws/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n",
      "download: s3://dsoaws/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n",
      "download: s3://dsoaws/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n",
      "download: s3://dsoaws/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dsoaws/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 14208525 Jun 15 23:37 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这段代码的主要目的是从预训练的模型中加载一个模型，并且初始化一个针对这个模型的分词器。之后，使用这个预训练模型和分词器创建一个PEFT模型。\n",
    "\n",
    "# 导入PEFT模型和配置类\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 使用\"google/flan-t5-base\"作为预训练模型，加载一个用于序列到序列的学习模型\n",
    "# 并指定模型使用的数据类型为bfloat16\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "# 从同样的预训练模型\"google/flan-t5-base\"中加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# 从本地路径'./peft-dialogue-summary-checkpoint-from-s3/'加载预训练的PEFT模型\n",
    "# 并指定模型使用的数据类型为bfloat16\n",
    "# 通过设置is_trainable参数为False，指定模型在接下来的使用中不会被训练\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# 调用print_number_of_trainable_model_parameters函数，打印PEFT模型中的可训练参数的数量\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  对模型进行定性评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Talk to a computer expert.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的主目的是使用三个不同的模型（原始模型、指导模型和PEFT模型）对同一段对话进行摘要，并将结果与人工摘要进行比较\n",
    "\n",
    "# 选择要摘要的对话的索引\n",
    "index = 200\n",
    "\n",
    "# 从数据集的测试集中获取对话文本\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "\n",
    "# 从数据集的测试集中获取人工摘要\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# 构建模型输入的提示\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "# 使用分词器将提示转换为模型可以接受的输入形式\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# 使用原始模型生成摘要，设置最大新令牌数为200，束宽为1\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# 使用分词器将模型输出的ID解码为文本\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 使用指导模型生成摘要\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# 解码指导模型的输出\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 使用PEFT模型生成摘要\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# 解码PEFT模型的输出\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印出不同模型的摘要结果以及人工摘要\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行定量评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:38<00:00,  3.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n",
       "      <td>Employees are now allowed to use the Instant Messaging system.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n",
       "      <td>#Person1#: I'm sorry, Ms. Dawson. I'm going to give you a memo.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.</td>\n",
       "      <td>#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. #Person1#: It should apply to all communications, not only intra-office communications, but also external communications. #Person2#: It should apply to external communications. #Person1#: It should apply to internal and external communications. #Person2#: Yes, it should apply to internal and external communications. #Person2#: It should apply to internal and external communica...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.</td>\n",
       "      <td>The traffic jam at the intersection of Carrefour and Carrefour is really bad.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.</td>\n",
       "      <td>#Person2#: I'm stuck in traffic again. #Person1#: I'm stuck in traffic again. #Person2#: I'm stuck in traffic again. #Person1#: I'm a bit afraid of the traffic. #Person2#: I'm going to have to consider taking public transport. #Person2#: I'm going to try to get a subway to work. #Person1#: I'm going to try to take the subway. #Person2#: I'm going to try to try to bike to work. #Person1#: I'm going to try to get a bike to work. #Person2#: I'm going to try to get a bike to work.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.</td>\n",
       "      <td>People are talking about the traffic problems in their lives.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.</td>\n",
       "      <td>#Person1: Happy Birthday, Brian!</td>\n",
       "      <td>Brian's birthday is coming. #Person1# invites Brian to have a dance and Brian compliments #Person1#'s looks. Brian thinks #Person1# looks great and invites #Person1# to have a drink together.</td>\n",
       "      <td>Brian remembers his birthday and invites #Person1# to the party. Brian is popular with everyone and looks pretty today. #Person1# and Brian will have a drink together to celebrate Brian's birthday.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                          human_baseline_summaries  \\\n",
       "0                                              Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n",
       "1  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n",
       "2                                  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.   \n",
       "3                                                       #Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.   \n",
       "4                                                                                      #Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.   \n",
       "5                                                                            #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.   \n",
       "6                                                                                            #Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.   \n",
       "7                                                                                         #Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.   \n",
       "8                                                                                 #Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched   \n",
       "9                                                                                                       #Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              original_model_summaries  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                       Employees are now allowed to use the Instant Messaging system.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                      #Person1#: I'm sorry, Ms. Dawson. I'm going to give you a memo.   \n",
       "2  #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. #Person1#: It should apply to all communications, not only intra-office communications, but also external communications. #Person2#: It should apply to external communications. #Person1#: It should apply to internal and external communications. #Person2#: Yes, it should apply to internal and external communications. #Person2#: It should apply to internal and external communica...   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                        The traffic jam at the intersection of Carrefour and Carrefour is really bad.   \n",
       "4                    #Person2#: I'm stuck in traffic again. #Person1#: I'm stuck in traffic again. #Person2#: I'm stuck in traffic again. #Person1#: I'm a bit afraid of the traffic. #Person2#: I'm going to have to consider taking public transport. #Person2#: I'm going to try to get a subway to work. #Person1#: I'm going to try to take the subway. #Person2#: I'm going to try to try to bike to work. #Person1#: I'm going to try to get a bike to work. #Person2#: I'm going to try to get a bike to work.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                        People are talking about the traffic problems in their lives.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Masha and Hero are getting divorced.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Masha and Hero are getting divorced.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Masha and Hero are getting divorced.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     #Person1: Happy Birthday, Brian!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                    instruct_model_summaries  \\\n",
       "0  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.   \n",
       "1  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.   \n",
       "2  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos and the use of Instant Message programs by employees during working hours is strictly prohibited. #Person1# wants to change the communication methods and Ms. Dawson tells #Person1# it applies to internal and external communications.   \n",
       "3                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.   \n",
       "4                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.   \n",
       "5                                                                                                                                                                                                                                     #Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# agrees.   \n",
       "6                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.   \n",
       "7                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.   \n",
       "8                                                                                                                                                                                                                                        Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.   \n",
       "9                                                                                                                                                                                                                                            Brian's birthday is coming. #Person1# invites Brian to have a dance and Brian compliments #Person1#'s looks. Brian thinks #Person1# looks great and invites #Person1# to have a drink together.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                    peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.  \n",
       "3                                                                           #Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.  \n",
       "4                                                                           #Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.  \n",
       "5                                                                           #Person2# got stuck in traffic and #Person1# suggests #Person2# start taking public transport system to work. #Person2# thinks it's better for the environment and #Person2# will miss having freedom with a car. #Person1# suggests biking to work when it's nicer outside.  \n",
       "6                                                                                                                       Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.  \n",
       "7                                                                                                                       Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.  \n",
       "8                                                                                                                       Kate tells #Person2# Masha and Hero are getting divorced. #Person2# thinks it's surprising because they are having a separation for 2 months and filed for divorce. #Person2# thinks it's the change from all the back stepping.  \n",
       "9                                                                                                                                                  Brian remembers his birthday and invites #Person1# to the party. Brian is popular with everyone and looks pretty today. #Person1# and Brian will have a drink together to celebrate Brian's birthday.  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这段代码的主要目的是对数据集中的前十个对话样本进行摘要，并将这些摘要以及对应的人工摘要存储在一个Pandas DataFrame中。\n",
    "\n",
    "# 从测试数据集中获取前十个对话样本\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "\n",
    "# 获取这些对话的人工摘要\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# 创建空列表，用于存储不同模型生成的摘要\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "# 使用tqdm函数创建一个进度条，并对每一个对话样本进行处理\n",
    "for idx, dialogue in enumerate(tqdm(dialogues)):\n",
    "    # 创建模型输入的提示\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "    # 使用分词器将提示转换为模型可以接受的输入形式\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # 从人工摘要列表中获取当前对话的人工摘要\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    # 使用原始模型生成摘要，并解码为文本\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 使用指导模型生成摘要，并解码为文本\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 使用PEFT模型生成摘要，并解码为文本\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 将生成的摘要添加到对应的列表中\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "# 使用zip函数将人工摘要和各个模型的摘要配对\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "# 使用Pandas创建一个DataFrame，每一列代表一个摘要来源\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "\n",
    "# 显示这个DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.28937301301960516, 'rouge2': 0.11499567424080169, 'rougeL': 0.24994683805358164, 'rougeLsum': 0.25145160436685865}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.3725351062275605, 'rouge2': 0.12138811933618107, 'rougeL': 0.27620639623170606, 'rougeLsum': 0.2758134870822362}\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的主要目标是使用ROUGE（Recall-Oriented Understudy for Gisting Evaluation）指标对三个模型（原始模型，指导模型，PEFT模型）生成的摘要进行评估，并将结果打印出来。\n",
    "\n",
    "# 加载ROUGE评估工具\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# rouge.compute函数接受以下参数：\n",
    "# - predictions：模型生成的摘要。\n",
    "# - references：参考摘要，这里使用的是人工摘要。\n",
    "# - use_aggregator：如果为True，将返回所有评分的平均值和95%置信区间。\n",
    "# - use_stemmer：如果为True，将在计算ROUGE分数前对预测和参考摘要进行词干化。\n",
    "\n",
    "# use_aggregator：该参数决定了是否对所有评分的结果进行聚合。\n",
    "# 如果设置为 True，那么结果将包含所有评分的平均值（mean）以及一个 95% 的置信区间（confidence interval）。\n",
    "# 置信区间是一种用于估计某个参数的区间，如果样本足够大，那么 95% 的概率下，参数的真实值会在这个区间内。\n",
    "\n",
    "# use_stemmer：该参数决定了在计算 ROUGE 分数之前，是否对预测和参考摘要进行词干化（stemming）。\n",
    "# 词干化是一种处理单词的方式，将单词简化为其词干或者词根形式，例如，“running”、“runner”和“ran”经过词干化后都可能变为“run”。\n",
    "# 这样可以帮助评估过程忽略单词的具体形式，只关注其基本含义。如果设置为 True，那么在计算评分之前，会先对预测和参考摘要进行词干化处理。\n",
    "\n",
    "# 对原始模型生成的摘要进行评估，参考标准为人工摘要\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# 对指导模型生成的摘要进行评估\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# 对PEFT模型生成的摘要进行评估\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# 打印三个模型的评估结果\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的主要目标是从 results 的 DataFrame 中提取出摘要，\n",
    "# 并使用 ROUGE（Recall-Oriented Understudy for Gisting Evaluation）指标来评估由三个模型（原始模型，指导模型，PEFT模型）生成的摘要。\n",
    "# 打印出了每个模型的评估结果。\n",
    "\n",
    "# 从 DataFrame 'results' 中提取人工生成的基线摘要\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "\n",
    "# 从 DataFrame 'results' 中提取原始模型生成的摘要\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "\n",
    "# 从 DataFrame 'results' 中提取指导模型生成的摘要\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "# 从 DataFrame 'results' 中提取 PEFT 模型生成的摘要\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "# 使用 ROUGE 指标对原始模型生成的摘要进行评估\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,  # 原始模型生成的摘要\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],  # 人工生成的参考摘要\n",
    "    use_aggregator=True,  # 返回所有评分的平均值和 95% 置信区间\n",
    "    use_stemmer=True,  # 在计算 ROUGE 分数之前对预测和参考摘要进行词干化\n",
    ")\n",
    "\n",
    "# 使用 ROUGE 指标对指导模型生成的摘要进行评估\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,  # 指导模型生成的摘要\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],  # 人工生成的参考摘要\n",
    "    use_aggregator=True,  # 返回所有评分的平均值和 95% 置信区间\n",
    "    use_stemmer=True,  # 在计算 ROUGE 分数之前对预测和参考摘要进行词干化\n",
    ")\n",
    "\n",
    "# 使用 ROUGE 指标对 PEFT 模型生成的摘要进行评估\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,  # PEFT 模型生成的摘要\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],  # 人工生成的参考摘要\n",
    "    use_aggregator=True,  # 返回所有评分的平均值和 95% 置信区间\n",
    "    use_stemmer=True,  # 在计算 ROUGE 分数之前对预测和参考摘要进行词干化\n",
    ")\n",
    "\n",
    "# 打印三个模型的评估结果\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 17.47%\n",
      "rouge2: 8.73%\n",
      "rougeL: 12.36%\n",
      "rougeLsum: 12.34%\n"
     ]
    }
   ],
   "source": [
    "# 这段代码的目的是计算 PEFT 模型在 ROUGE 评分上相对于原始模型的绝对百分比改进，并将每个 ROUGE 指标的改进打印出来。\n",
    "\n",
    "# 打印说明信息\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "# 计算 PEFT 模型的 ROUGE 分数相对于原始模型的绝对改进\n",
    "# 首先将每个模型的 ROUGE 分数从字典中提取出来，转换为 numpy 数组\n",
    "# 然后计算两个数组的差，得到每个 ROUGE 指标的改进\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "\n",
    "# 循环遍历每个 ROUGE 指标及其对应的改进，打印改进的百分比\n",
    "# 字符串格式化用于保留两位小数\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.35%\n",
      "rouge2: -1.70%\n",
      "rougeL: -1.34%\n",
      "rougeLsum: -1.35%\n"
     ]
    }
   ],
   "source": [
    "# 打印说明信息\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "# 计算 PEFT 模型的 ROUGE 分数相对于指导模型的绝对改进\n",
    "# 首先将每个模型的 ROUGE 分数从字典中提取出来，转换为 numpy 数组\n",
    "# 然后计算两个数组的差，得到每个 ROUGE 指标的改进\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "\n",
    "# 循环遍历每个 ROUGE 指标及其对应的改进，打印改进的百分比\n",
    "# 字符串格式化用于保留两位小数\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
