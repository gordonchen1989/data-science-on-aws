{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune 对话摘要模型\n",
    "\n",
    "![Pipeline](./img/generative_ai_pipeline_rlhf_plus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "## Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"img/kernel_set_up.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"img/w3_kernel_and_instance_type.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=33229983744, available=27538239488, percent=17.1, used=5322313728, free=436244480, active=4309553152, inactive=25311002624, buffers=1634304, cached=27469791232, shared=1236992, slab=2485346304)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "notebook_memory = psutil.virtual_memory()\n",
    "print(notebook_memory)\n",
    "\n",
    "if notebook_memory.total < 32 * 1000 * 1000 * 1000:\n",
    "    print('*******************************************')    \n",
    "    print('YOU ARE NOT USING THE CORRECT INSTANCE TYPE')\n",
    "    print('PLEASE CHANGE INSTANCE TYPE TO  m5.2xlarge ')\n",
    "    print('*******************************************')\n",
    "else:\n",
    "    correct_instance_type=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_checkpoint\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the PREPARE section before you continue.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r local_data_processed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    local_data_processed_path\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the PREPARE section before you continue.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data-summarization-processed/\n"
     ]
    }
   ],
   "source": [
    "print(local_data_processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入transformers库中的几个重要模块\n",
    "# AutoTokenizer：自动标记器，用于加载适合预训练模型的标记器\n",
    "# AutoModelForSeq2SeqLM：用于加载适合序列到序列语言模型的预训练模型\n",
    "# TrainingArguments：训练参数，用于设置训练过程中的各种参数\n",
    "# Trainer：训练器，用于进行模型训练的类\n",
    "# GenerationConfig：生成配置，用于设置模型生成文本的参数\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, GenerationConfig\n",
    "\n",
    "# 导入datasets库中的load_dataset函数，用于加载数据集\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 导入datasets库，用于使用库中的其他功能\n",
    "import datasets\n",
    "\n",
    "# 导入PyTorch库，这是一个开源的深度学习平台\n",
    "import torch\n",
    "\n",
    "# 导入time模块，用于处理和格式化时间\n",
    "import time\n",
    "\n",
    "# 导入evaluate模块，这可能是一个用于模型评估的自定义模块\n",
    "import evaluate\n",
    "\n",
    "# 导入numpy库，这是一个用于处理大型多维数组和矩阵的库，提供了大量的数学函数来操作这些数组\n",
    "import numpy as np\n",
    "\n",
    "# 设置模型运行所使用的设备\n",
    "# 如果有可用的CUDA设备（也就是GPU），就使用CUDA设备，否则就使用CPU\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 加载HuggingFace模型\n",
    "\n",
    "我们可以直接从HuggingFace加载预训练的Flan-T5模型。请注意，我们将使用[base version](https://huggingface.co/google/flan-t5-base)的flan。此模型版本有大约2.47亿的模型参数，相比其他大型语言模型来说较小。如果想要获得更高质量的结果，我们建议研究此模型的更大版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用AutoTokenizer的from_pretrained方法加载预训练模型对应的标记器\n",
    "# model_checkpoint是预训练模型的名字或路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 使用AutoModelForSeq2SeqLM的from_pretrained方法加载预训练的序列到序列语言模型\n",
    "# model_checkpoint是预训练模型的名字或路径\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Model Parameters: 247577856\n"
     ]
    }
   ],
   "source": [
    "# 使用列表推导式统计模型中的总参数数量\n",
    "# p.numel()方法返回Tensor中元素的个数\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 打印模型中的总参数数量\n",
    "print(f'Total Number of Model Parameters: {params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载处理后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 加载数据集\n",
    "\n",
    "我们已经处理过的DialogSum数据集可以直接从我们的本地目录加载。这个数据集中有大约15k个对话示例，并且这些数据集都有相应的人工摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/data-summarization-processed-501c81c8367fa59b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48614e7fde5d411cbf77683f61bbb775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 13014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 723\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 723\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用load_dataset函数加载数据集\n",
    "# local_data_processed_path是数据集的路径\n",
    "# data_files是一个字典，键是数据集的名称（'train'、'test'、'validation'），值是对应的文件路径\n",
    "# with_format(\"torch\")方法用于将数据集格式化为PyTorch的Tensor格式\n",
    "tokenized_dataset = load_dataset(\n",
    "    local_data_processed_path,\n",
    "    data_files={'train': 'train/*.parquet', 'test': 'test/*.parquet', 'validation': 'validation/*.parquet'}\n",
    ").with_format(\"torch\")\n",
    "\n",
    "# 打印加载的数据集的内容\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 在微调之前，用 Zero-Shot Prompts测试模型\n",
    "\n",
    "在下面的示例中，我们强调了与数据集中提供的基线摘要相比，模型的摘要能力是不足的。你可以看到，与基线摘要相比，模型在总结对话时存在困难，但它确实从文本中提取出了一些重要信息，这表明可以对模型进行微调以完成手头的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "--------------------------\n",
      "Summarize the following conversation. \n",
      "#Person1#: Hello. My name is John Sandals, and I've got a reservation. \n",
      "#Person2#: May I see some identification, sir, please? \n",
      "#Person1#: Sure. Here you are. \n",
      "#Person2#: Thank you so much. Have you got a credit card, Mr. Sandals? \n",
      "#Person1#: I sure do. How about American Express? \n",
      "#Person2#: Unfortunately, at the present time we take only MasterCard or VISA. \n",
      "#Person1#: No American Express? Okay, here's my VISA. \n",
      "#Person2#: Thank you, sir. You'll be in room 507, nonsmoking, with a queen-size bed. Do you approve, sir? \n",
      "#Person1#: Yeah, that'll be fine. \n",
      "#Person2#: That's great. This is your key, sir. If you need anything at all, anytime, just dial zero. Summary: \n",
      "--------------------------\n",
      "\n",
      "Original Model Response: John Sandals has a reservation for a room at the Venetian Hotel in Las Vegas.\n",
      "Baseline Summary : John Sandals has got a reservation. #Person1# asks for his identification and credit card and helps his check-in.\n"
     ]
    }
   ],
   "source": [
    "# 加载测试集中的一条数据，使用模型生成一个摘要，并将原始输入、模型生成的摘要和基线摘要打印出来。\n",
    "\n",
    "# 设置要加载的数据的索引\n",
    "idx = 2\n",
    "\n",
    "# 使用tokenizer的decode方法将输入的标记解码为文本，skip_special_tokens=True表示跳过特殊标记\n",
    "diag = tokenizer.decode(tokenized_dataset['test'][idx]['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "# 将解码后的文本重新编码为模型输入\n",
    "model_input = tokenizer(diag, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 使用tokenizer的decode方法将标签的标记解码为文本，这是基线摘要\n",
    "summary = tokenizer.decode(tokenized_dataset['test'][idx]['labels'], skip_special_tokens=True)\n",
    "\n",
    "# 使用模型的generate方法生成摘要，设置最大新标记数量为200\n",
    "original_outputs = model.to('cpu').generate(model_input, GenerationConfig(max_new_tokens=200))\n",
    "\n",
    "# 将生成的摘要解码为文本\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 将输入的文本中的'#'字符替换为'\\n#'，用于格式化输出\n",
    "diag_print = diag.replace(' #',' \\n#')\n",
    "\n",
    "# 打印原始输入\n",
    "print(f\"Prompt:\\n--------------------------\\n{diag_print}\\n--------------------------\")\n",
    "\n",
    "# 打印模型生成的摘要\n",
    "print(f'\\nOriginal Model Response: {original_text_output}')\n",
    "\n",
    "# 打印基线摘要\n",
    "print(f'Baseline Summary : {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调模型\n",
    "\n",
    "数据集已经预处理完毕，我们可以利用HuggingFace内置的 `Trainer` 类来微调我们的模型以适应手头的任务。请注意，全模型的训练在GPU上需要几个小时，所以为了节省时间，我们提供了一个已经在没有下采样的情况下训练了10个epoch的模型的检查点。如果你有时间尝试自己完全训练模型，请参考注释了解如何更改代码。如果你打算在GPU机器上训练，我们已经提供了一个 `ml.g5.xlarge` 实例的检查点作为起点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/13014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 为了节省实验室的时间，我们将对我们的数据集进行子采样\n",
    "# 如果你想花时间全面训练一个模型，可以自由修改这个子采样以创建一个更大的数据集\n",
    "# lambda example, indice: indice % 100 == 0是一个匿名函数，该函数返回True如果索引值（indice）是100的整数倍，否则返回False\n",
    "# filter函数将只保留那些索引值是100的倍数的样本，也就是说，每100个样本，只取一个\n",
    "# with_indices=True参数指示filter函数在调用参数函数时传递样本的索引\n",
    "sample_tokenized_dataset = tokenized_dataset.filter(lambda example, indice: indice % 100 == 0, with_indices=True)\n",
    "\n",
    "# 设置模型的输出目录，名为“diag-summary-training-”后接当前时间戳\n",
    "output_dir = f'./diag-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # 指定模型输出的目录\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略为每个epoch结束时进行一次评估\n",
    "    save_strategy=\"epoch\",  # 设置保存策略为每个epoch结束时保存一次模型\n",
    "    learning_rate=1e-5,  # 设置学习率\n",
    "    num_train_epochs=1,  # 设置训练的epoch数量\n",
    "    # num_train_epochs=10, # 当你不在实验室并且有更多的时间去试验时，可以使用更高的epoch数量\n",
    "    # per_device_train_batch_size和per_device_eval_batch_size是设定在训练和评估过程中每个设备上处理的数据批次（batch）的大小。\n",
    "    # per_device_train_batch_size=4：这个参数设置了每个设备在训练过程中一次处理的样本数量为4。这意味着在每一步训练（一个步骤对应于一次参数更新）中，模型将基于4个样本的平均损失进行学习。选择合适的批次大小对于模型的训练很重要，因为它会影响模型的学习效率和训练的稳定性。\n",
    "    # per_device_eval_batch_size=4：这个参数设置了在评估过程中每个设备一次处理的样本数量为4。在评估过程中，模型会计算这4个样本的平均损失或者其他评估指标，以估计模型在验证集上的性能。\n",
    "    # 这两个参数的选择取决于你的硬件配置（主要是GPU内存）和数据集的大小。如果选择的批次太大，可能会导致内存溢出；如果批次太小，可能会影响训练的速度和模型的性能。所以在实际使用中，需要根据具体情况来调整这两个参数。\n",
    "    per_device_train_batch_size=4,  # 设置每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=4,  # 设置每个设备的评估批次大小\n",
    "    weight_decay=0.01,  # 设置权重衰减值\n",
    ")\n",
    "\n",
    "# 创建训练器\n",
    "trainer = Trainer(\n",
    "    model=model,  # 指定模型\n",
    "    args=training_args,  # 指定训练参数\n",
    "    train_dataset=sample_tokenized_dataset['train'],  # 指定训练数据集\n",
    "    eval_dataset=sample_tokenized_dataset['validation']  # 指定评估数据集\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 07:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>35.455990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33, training_loss=38.12038722182765, metrics={'train_runtime': 451.36, 'train_samples_per_second': 0.29, 'train_steps_per_second': 0.073, 'total_flos': 89703213170688.0, 'train_loss': 38.12038722182765, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练过的模型和原始模型\n",
    "\n",
    "模型完成训练后，我们加载来自HuggingFace的原始模型和经过微调的模型，进行一些比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/generation_config.json to flan-dialogue-summary-checkpoint/generation_config.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/config.json to flan-dialogue-summary-checkpoint/config.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/scheduler.pt to flan-dialogue-summary-checkpoint/scheduler.pt\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/rng_state.pth to flan-dialogue-summary-checkpoint/rng_state.pth\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/trainer_state.json to flan-dialogue-summary-checkpoint/trainer_state.json\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/training_args.bin to flan-dialogue-summary-checkpoint/training_args.bin\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/pytorch_model.bin to flan-dialogue-summary-checkpoint/pytorch_model.bin\n",
      "download: s3://dsoaws/models/flan-dialogue-summary-checkpoint/optimizer.pt to flan-dialogue-summary-checkpoint/optimizer.pt\n"
     ]
    }
   ],
   "source": [
    "# 从Amazon S3复制模型检查点到本地\n",
    "!aws s3 cp --recursive s3://dsoaws/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 如果你有自己训练的模型，想与我们的模型进行对比，可以修改以下代码行\n",
    "# 使其包含你的检查点目录\n",
    "\n",
    "supervised_fine_tuned_model_path = \"./flan-dialogue-summary-checkpoint\"\n",
    "# supervised_fine_tuned_model_path = f\"./{output_dir}/<put-your-checkpoint-dir-here>\"\n",
    "\n",
    "# 从预先训练好的模型路径加载微调过的模型\n",
    "tuned_model = AutoModelForSeq2SeqLM.from_pretrained(supervised_fine_tuned_model_path)\n",
    "\n",
    "# 从预训练模型的检查点加载模型\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'supervised_fine_tuned_model_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store supervised_fine_tuned_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调后的 Zero Shot 推理\n",
    "\n",
    "像许多GenAI应用一样，采用定性方法，问自己这样的问题：\"我的模型是否按照预期的方式运行？\"通常是一个好的起点。在下面的例子中（我们用这个例子开始了这个笔记本），你可以看到经过微调的模型能够创建一个合理的对话摘要，相比之下，原始模型无法理解对模型的要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "--------------------------\n",
      "Summarize the following conversation. \n",
      "#Person1#: Hello. My name is John Sandals, and I've got a reservation. \n",
      "#Person2#: May I see some identification, sir, please? \n",
      "#Person1#: Sure. Here you are. \n",
      "#Person2#: Thank you so much. Have you got a credit card, Mr. Sandals? \n",
      "#Person1#: I sure do. How about American Express? \n",
      "#Person2#: Unfortunately, at the present time we take only MasterCard or VISA. \n",
      "#Person1#: No American Express? Okay, here's my VISA. \n",
      "#Person2#: Thank you, sir. You'll be in room 507, nonsmoking, with a queen-size bed. Do you approve, sir? \n",
      "#Person1#: Yeah, that'll be fine. \n",
      "#Person2#: That's great. This is your key, sir. If you need anything at all, anytime, just dial zero. Summary: \n",
      "--------------------------\n",
      "Flan-T5 response: John Sandals has a reservation for a room at the Venetian Hotel in Las Vegas.\n",
      "Our instruct-tuned response (on top of Flan-T5): John Sandals has a reservation and checks in with his VISA. #Person2# helps him to check in and approves his reservation.\n",
      "Baseline summary from original dataset: John Sandals has got a reservation. #Person1# asks for his identification and credit card and helps his check-in.\n"
     ]
    }
   ],
   "source": [
    "# 设置要测试的样本的索引\n",
    "idx = 2\n",
    "\n",
    "# 使用tokenizer解码测试集中该索引对应的输入数据，并跳过特殊的标记（如：[CLS], [SEP]）\n",
    "diag = tokenizer.decode(tokenized_dataset['test'][idx]['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "# 使用tokenizer将解码后的输入数据编码为模型可以接收的格式\n",
    "model_input = tokenizer(diag, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 使用tokenizer解码测试集中该索引对应的标签数据（即：真实的摘要），并跳过特殊标记\n",
    "summary = tokenizer.decode(tokenized_dataset['test'][idx]['labels'], skip_special_tokens=True)\n",
    "\n",
    "# 使用原始模型生成摘要，限制最多生成200个新的标记，并使用束搜索来提高生成质量\n",
    "original_outputs = model.to('cpu').generate(\n",
    "    model_input,\n",
    "    GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    ")\n",
    "\n",
    "# 使用微调过的模型生成摘要，限制最多生成200个新的标记，并使用束搜索来提高生成质量\n",
    "outputs = tuned_model.to('cpu').generate(\n",
    "    model_input,\n",
    "    GenerationConfig(max_new_tokens=200, num_beams=1,),\n",
    ")\n",
    "\n",
    "# 使用tokenizer解码生成的摘要，并跳过特殊标记\n",
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 将输入数据中的 '#' 替换为 '\\n#'，用于更美观地打印\n",
    "diag_print = diag.replace(' #',' \\n#')\n",
    "\n",
    "# 打印输入数据、原始模型和微调模型生成的摘要，以及原始数据集中的真实摘要\n",
    "print(f\"Prompt:\\n--------------------------\\n{diag_print}\\n--------------------------\")\n",
    "print(f'Flan-T5 response: {original_text_output}')\n",
    "print(f'Our instruct-tuned response (on top of Flan-T5): {text_output}')\n",
    "print(f'Baseline summary from original dataset: {summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用ROUGE指标进行的定量结果\n",
    "\n",
    "[ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) 有助于量化模型产生的摘要的有效性。它将摘要与通常由人类创建的\"基线\"摘要进行比较。虽然不完美，但它确实给出了我们通过微调所取得的摘要效果整体提升的一种指示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72450e0ada544916938503a9f01b6576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载 \"rouge\" 评估工具。这里的 \"rouge\" 指的是一种用于评估自动文本摘要质量的指标。它通过比较生成的摘要和参考摘要之间的重叠来计算得分。\n",
    "# 加载评估指标 \"rouge\"\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一些例子来说明ROUGE评估的工作方式：\n",
    "\n",
    "假设我们有一个参考摘要（真实摘要）和一个生成摘要（由模型生成）。\n",
    "\n",
    "参考摘要：\n",
    "```\n",
    "The cat sat on the mat.\n",
    "```\n",
    "生成摘要：\n",
    "```\n",
    "The cat is sitting on the mat.\n",
    "```\n",
    "如果我们计算ROUGE-1（即计算一元组的重叠），我们看到这两个摘要有5个共享的词（\"The\", \"cat\", \"on\", \"the\", \"mat\"）。所以ROUGE-1的召回率就是5/6=0.83，精确率就是5/7=0.71，F1得分是2*(0.83*0.71)/(0.83+0.71)=0.77。\n",
    "\n",
    "如果我们计算ROUGE-2（即计算二元组的重叠），共享的二元组有3个（\"The cat\", \"on the\", \"the mat\"），所以ROUGE-2的召回率就是3/5=0.6，精确率就是3/6=0.5，F1得分是2*(0.6*0.5)/(0.6+0.5)=0.54。\n",
    "\n",
    "注意，ROUGE通常更关注召回率而不是精确率，因为在摘要任务中，我们更关心生成的摘要能覆盖多少参考摘要的信息。\n",
    "\n",
    "另外，还有一些其他的ROUGE评估方法，例如ROUGE-L，它计算的是最长公共子序列（LCS），和ROUGE-S，它计算的是跳跃二元组（skip-bigram）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估部分摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 由于时间原因，我们只生成每个模型的少数几个摘要\n",
    "# 在实验之外，一个好的练习是增加生成的验证摘要的数量\n",
    "# 从测试数据集中选择前10条对话\n",
    "dialogues = tokenized_dataset['test'][0:10]['input_ids']\n",
    "# 选择相应的摘要作为基线摘要\n",
    "baseline_summaries = tokenized_dataset['test'][0:10]['labels']\n",
    "\n",
    "# 解码原始摘要\n",
    "human_baseline_summaries = []\n",
    "# 对于每个基线摘要，使用tokenizer进行解码，并忽略特殊标记\n",
    "for base_summary in baseline_summaries:\n",
    "    human_baseline_summaries.append(tokenizer.decode(base_summary, skip_special_tokens=True))\n",
    "\n",
    "# 生成摘要\n",
    "# 使用原始模型生成摘要，每个摘要的最大长度设为200个标记\n",
    "original_outputs = model.generate(dialogues, GenerationConfig(max_new_tokens=200))\n",
    "# 使用调优后的模型生成摘要，每个摘要的最大长度设为200个标记\n",
    "tuned_outputs = tuned_model.generate(dialogues, GenerationConfig(max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 在列表中存储摘要\n",
    "original_model_summaries = []  # 存储原始模型生成的摘要\n",
    "tuned_model_summaries = []  # 存储调优后的模型生成的摘要\n",
    "\n",
    "# 解码所有摘要\n",
    "# 使用zip函数同时遍历原始模型和调优模型生成的摘要\n",
    "for original_summary, tuned_summary in zip(original_outputs, tuned_outputs):\n",
    "    # 使用tokenizer对原始模型生成的摘要进行解码，忽略特殊标记，并添加到对应的列表中\n",
    "    original_model_summaries.append(tokenizer.decode(original_summary, skip_special_tokens=True))\n",
    "    # 使用tokenizer对调优模型生成的摘要进行解码，忽略特殊标记，并添加到对应的列表中\n",
    "    tuned_model_summaries.append(tokenizer.decode(tuned_summary, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用ROUGE评估指标计算原始模型生成的摘要与人类生成的摘要之间的相似度\n",
    "original_results = rouge.compute(\n",
    "    # 预测的摘要，即原始模型生成的摘要\n",
    "    predictions=original_model_summaries,\n",
    "    # 参考的摘要，即人类生成的摘要\n",
    "    references=human_baseline_summaries,\n",
    "    # 使用聚合器，这样可以计算多个摘要的平均ROUGE得分\n",
    "    use_aggregator=True,\n",
    "    # 使用词干处理，这样可以将单词的各种形态（如复数、过去式等）统一为其基本形式进行比较\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用 ROUGE 评估指标计算调优后的模型生成的摘要与人类生成的摘要之间的相似度\n",
    "tuned_results = rouge.compute(\n",
    "    # 预测的摘要，即调优后的模型生成的摘要\n",
    "    predictions=tuned_model_summaries,\n",
    "    # 参考的摘要，即人类生成的摘要\n",
    "    references=human_baseline_summaries,\n",
    "    # 使用聚合器，这样可以计算多个摘要的平均 ROUGE 得分\n",
    "    use_aggregator=True,\n",
    "    # 使用词干处理，这样可以将单词的各种形态（如复数、过去式等）统一为其基本形式进行比较\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.23899693678641046,\n",
       " 'rouge2': 0.08932806324110672,\n",
       " 'rougeL': 0.21776705653021444,\n",
       " 'rougeLsum': 0.21325132275132275}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.5243400833160587,\n",
       " 'rouge2': 0.240785255433749,\n",
       " 'rougeL': 0.3960164115550142,\n",
       " 'rougeLsum': 0.396040090323604}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估完整的数据集\n",
    "\n",
    "\"diag-summary-training-results.csv\"文件包含了所有模型结果的预填充列表，我们可以使用它来评估更大部分的数据。结果显示，在所有的ROUGE指标上都有显著的改善！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入pandas库，用于数据处理和分析\n",
    "import pandas as pd\n",
    "\n",
    "# 使用pandas的read_csv函数读取CSV文件 \"diag-summary-training-results.csv\"，并将结果存储在results变量中\n",
    "results = pd.read_csv(\"diag-summary-training-results.csv\")\n",
    "\n",
    "# 从results中提取 'original_model_summaries' 这一列的值，将其转换为numpy数组，并存储在 original_model_summaries 变量中\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "\n",
    "# 从results中提取 'tuned_model_summaries' 这一列的值，将其转换为numpy数组，并存储在 tuned_model_summaries 变量中\n",
    "tuned_model_summaries = results['tuned_model_summaries'].values\n",
    "\n",
    "# 从results中提取 'human_baseline_summaries' 这一列的值，将其转换为numpy数组，并存储在 human_baseline_summaries 变量中\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用 ROUGE 评估指标计算原始模型生成的摘要与人类生成的摘要之间的相似度\n",
    "original_results = rouge.compute(\n",
    "    # 预测的摘要，即原始模型生成的摘要\n",
    "    predictions=original_model_summaries,\n",
    "    # 参考的摘要，即人类生成的摘要，但只选取了与原始模型生成的摘要数量相同的部分\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    # 使用聚合器，这样可以计算多个摘要的平均 ROUGE 得分\n",
    "    use_aggregator=True,\n",
    "    # 使用词干处理，这样可以将单词的各种形态（如复数、过去式等）统一为其基本形式进行比较\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_results = rouge.compute(\n",
    "    predictions=tuned_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(tuned_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.2334158581572823,\n",
       " 'rouge2': 0.07603964187010573,\n",
       " 'rougeL': 0.20145520923859048,\n",
       " 'rougeLsum': 0.20145899339006135}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.42161291557556113,\n",
       " 'rouge2': 0.18035380596301792,\n",
       " 'rougeL': 0.3384439349963909,\n",
       " 'rougeLsum': 0.33835653595561666}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 absolute percentage improvement after instruct fine-tuning: 18.82%\n",
      "rouge2 absolute percentage improvement after instruct fine-tuning: 10.43%\n",
      "rougeL absolute percentage improvement after instruct fine-tuning: 13.70%\n",
      "rougeLsum absolute percentage improvement after instruct fine-tuning: 13.69%\n"
     ]
    }
   ],
   "source": [
    "# 计算微调模型与原始模型在各个 ROUGE 评估指标上的得分差异\n",
    "improvement = (np.array(list(tuned_results.values())) - np.array(list(original_results.values())))\n",
    "\n",
    "# 遍历微调模型的评估结果\n",
    "for key, value in zip(tuned_results.keys(), improvement):\n",
    "    # 打印每个 ROUGE 评估指标的名称（key）以及在此指标上的改进程度（value），并将改进程度转换为百分比的形式\n",
    "    print(f'{key} absolute percentage improvement after instruct fine-tuning: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
